{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from attalos.imgtxt_algorithms.correlation.correlation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordvecs_dir = os.environ[\"WORDVECS_DIR\"]\n",
    "dataset_dir = os.environ[\"DATASET_DIR\"]\n",
    "octave_eval_dir = os.environ[\"OCTAVE_EVAL_DIR\"]\n",
    "img_dir = os.environ.get(\"IMG_DIR\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# W2V Word Vectors\n",
    "import word2vec\n",
    "w2v_vector_file = os.path.join(wordvecs_dir, \"text9Bvin.bin\")\n",
    "w2v_model = word2vec.load(w2v_vector_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2v_model.glove.most_similar(\"airplane\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GloVe Word Vectors\n",
    "from glove import Glove\n",
    "from attalos.imgtxt_algorithms.correlation.w2vwrapper import W2VWrapper\n",
    "glove_model = Glove.load_stanford(os.path.join(wordvecs_dir, \"glove.6B.200d.txt\"))\n",
    "w2v_model = W2VWrapper(glove_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(w2v_model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from oct2py import octave\n",
    "octave.addpath(octave_eval_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imdata = np.load(os.path.join(dataset_dir, \"iaprtc-12/iaprtc12-inria.npz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imdata = np.load(os.path.join(dataset_dir, \"espgame/espgame-inria.npz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xTr = imdata['xTr']\n",
    "xTe = imdata['xTe']\n",
    "yTr = imdata['yTr']\n",
    "yTe = imdata['yTe']\n",
    "D = imdata['D']\n",
    "train_ims = imdata['trainlist']\n",
    "test_ims = imdata['testlist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for idx, row in enumerate(yTe):\n",
    "    if row[1] == 1:\n",
    "        print idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Images of planes: \n",
    "1147\n",
    "1893\n",
    "2323\n",
    "2909\n",
    "2930\n",
    "3954\n",
    "4229\n",
    "4346\n",
    "4363\n",
    "4475\n",
    "4581\n",
    "4608\n",
    "4663\n",
    "4666\n",
    "4696\n",
    "4789\n",
    "4960\n",
    "5018\n",
    "5121\n",
    "5131\n",
    "5156\n",
    "5259\n",
    "5424\n",
    "5569\n",
    "5638\n",
    "5688\n",
    "5696\n",
    "5807\n",
    "5825\n",
    "5905\n",
    "5967\n",
    "6043\n",
    "6060\n",
    "6165\n",
    "6202\n",
    "6246\n",
    "6286\n",
    "6388\n",
    "6582\n",
    "6604\n",
    "6639\n",
    "6640\n",
    "6705\n",
    "6765\n",
    "6791\n",
    "7176\n",
    "7240\n",
    "7340\n",
    "7343\n",
    "7512\n",
    "7870\n",
    "7898\n",
    "8043\n",
    "8172\n",
    "8173\n",
    "8208\n",
    "8244\n",
    "8368\n",
    "8387\n",
    "8587\n",
    "8894\n",
    "8990\n",
    "8992\n",
    "9251\n",
    "9392\n",
    "9670\n",
    "9732\n",
    "9761\n",
    "9859\n",
    "9910\n",
    "10532\n",
    "10738\n",
    "10796\n",
    "10906\n",
    "10932\n",
    "11030\n",
    "11149\n",
    "11245\n",
    "11332\n",
    "11411\n",
    "11604\n",
    "11633\n",
    "11696\n",
    "11884\n",
    "12060\n",
    "12212\n",
    "12213\n",
    "12251\n",
    "12813\n",
    "13603\n",
    "13604\n",
    "13605\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print xTr.shape\n",
    "print yTr.shape\n",
    "print xTe.shape\n",
    "print yTe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter out labels not in W2V vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "to_remove = get_invalid_labels(D, w2v_model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "D = filter(lambda word: word not in to_remove.keys(), D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yTr = np.delete(yTr, to_remove.values(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yTe = np.delete(yTe, to_remove.values(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print yTr.shape\n",
    "print yTe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orig_yTr = yTr\n",
    "orig_yTe = yTe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_L = construct_W(w2v_model, D, dtype=np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correlation_arr = np.dot(W_L.T, W_L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correlation_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.stem(correlation_arr[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correlation_arr_normed = correlation_arr / np.linalg.norm(correlation_arr, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.stem(correlation_arr_normed[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = []\n",
    "for row in correlation_arr_normed:\n",
    "    n = 2\n",
    "    sorted_idxs = [i for i in row.argsort()[::-1]]\n",
    "    top_idxs = sorted_idxs[:n]\n",
    "    bottom_idxs = sorted_idxs[-n:]\n",
    "    z = np.zeros(row.shape)\n",
    "    for idx in np.concatenate([top_idxs, bottom_idxs]):\n",
    "        print \"%s (%s)\" % (idx, D[idx])\n",
    "        z[idx] = row[idx]\n",
    "    #v1 = w2v_model.get_vector(D[top_idxs[0]])\n",
    "    #v2 = w2v_model.get_vector(D[top_idxs[1]])\n",
    "    #print \"correlation: %s\" % np.dot(v1, v2)\n",
    "    print \"-------\"\n",
    "    tmp.append(z)\n",
    "correlation_arr_normed_nonlinear = np.asarray(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.stem(correlation_arr_normed_nonlinear[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#correlation_arr_normed_nonlinear = nonlinearity(correlation_arr_normed) #np.power(correlation_arr_normed, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plt.stem(scale(correlation_arr_normed_nonlinear)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final = scale2(correlation_arr_normed_nonlinear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.stem(final[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform yTr into correlation space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yTr = np.dot(orig_yTr, final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print xTr.shape\n",
    "print yTr.shape\n",
    "print xTe.shape\n",
    "print yTe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_idx = 5905"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(plt.imread(os.path.join(dataset_dir, \"iaprtc-12\", \"images\", \"images\", \"%s.jpg\" % train_ims[img_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.stem(orig_yTr[img_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.stem(yTr[img_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_yTr = np.copy(yTr)\n",
    "for row_idx, row in enumerate(filtered_yTr):\n",
    "    for idx, val in enumerate(row):\n",
    "        if idx not in np.where(orig_yTr[row_idx]>0)[0]:\n",
    "            row[idx] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.stem(filtered_yTr[img_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print [D[i] for i in np.where(orig_yTr[img_idx]>0)[0]]\n",
    "print [D[i]\n",
    "    for i in np.where(filtered_yTr[img_idx]>0)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yTr_scaled = scale2(orig_yTr) #scale2(filtered_yTr) #scale2(yTr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nan_idxs = []\n",
    "for idx, row in enumerate(yTr_scaled):\n",
    "    if np.isnan(row).any():\n",
    "        nan_idxs.append(idx)\n",
    "print nan_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for nan_idx in nan_idxs:\n",
    "    for idx, val in enumerate(yTr_scaled[nan_idx]):\n",
    "        yTr_scaled[nan_idx][idx] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.stem(yTr_scaled[img_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yTr = yTr_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sklearn's Linear Regression implementation\n",
    "from sklearn import linear_model\n",
    "n_jobs = -1 # -1 = all CPUs, default = 1\n",
    "linreg_model = linear_model.LinearRegression(n_jobs=n_jobs)\n",
    "linreg_model.fit(xTr, yTr)\n",
    "yHat = linreg_model.predict(xTe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Spark MLLib's Linear Regression implementation\n",
    "\"\"\"\n",
    "from pyspark.mllib.regression import LabeledPoint, LinearRegressionWithSGD, LinearRegressionModel\n",
    "from itertools import izip\n",
    "\n",
    "def spark_linear_regression(xTr, yTr, xTe):\n",
    "    for x, y in izip(xTr, yTr):\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sklearn Polynomial Regression\n",
    "\"\"\"\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_degrees = 2\n",
    "poly = PolynomialFeatures(degree=poly_degrees)\n",
    "xTr_poly = poly.fit_transform(xTr)\n",
    "xTe_poly = poly.fit_transform(xTe)\n",
    "n_jobs = -1 # -1 = all CPUs, default = 1\n",
    "linreg_model = linear_model.LinearRegression(n_jobs=n_jobs)\n",
    "linreg_model.fit(xTr, yTr)\n",
    "yHat = linreg_model.predict(xTe)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tensorflow Linear Regression\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import tflearn\n",
    "\n",
    "# Regression data\n",
    "X = [3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,7.042,10.791,5.313,7.997,5.654,9.27,3.1]\n",
    "Y = [1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,2.827,3.465,1.65,2.904,2.42,2.94,1.3]\n",
    "\n",
    "# Linear Regression graph\n",
    "input_ = tflearn.input_data(shape=[None])\n",
    "linear = tflearn.single_unit(input_)\n",
    "regression = tflearn.regression(linear, optimizer='sgd', loss='mean_square',\n",
    "                                metric='R2', learning_rate=0.01)\n",
    "m = tflearn.DNN(regression)\n",
    "m.fit(X, Y, n_epoch=1000, show_metric=True, snapshot_epoch=False)\n",
    "\n",
    "print(\"\\nRegression result:\")\n",
    "print(\"Y = \" + str(m.get_weights(linear.W)) +\n",
    "      \".X + \" + str(m.get_weights(linear.b)))\n",
    "\n",
    "print(\"\\nTest prediction for y = 3.2 and y = 4.5:\")\n",
    "print(m.predict([3.2, 4.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tensorflow Linear Regression \n",
    "from __future__ import absolute_import, division, print_function\n",
    "import tflearn\n",
    "\n",
    "# Linear Regression graph\n",
    "input_ = tflearn.input_data(shape=[None, xTr.shape[1]])\n",
    "labels = tf.placeholder(tf.float32, [None, yTr.shape[1]], name='labels')    \n",
    "linear = tflearn.single_unit(input_)\n",
    "regression = tflearn.regression(linear, placeholder=labels, optimizer='sgd', loss='mean_square',\n",
    "                                metric='R2', learning_rate=0.01)\n",
    "m = tflearn.DNN(regression)\n",
    "m.fit(xTr, yTr, n_epoch=1000, show_metric=True, snapshot_epoch=False)\n",
    "\n",
    "print(\"\\nRegression result:\")\n",
    "print(\"Y = \" + str(m.get_weights(linear.W)) +\n",
    "      \".X + \" + str(m.get_weights(linear.b)))\n",
    "\n",
    "#print(\"\\nTest prediction for y = 3.2 and y = 4.5:\")\n",
    "#print(m.predict([3.2, 4.5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## tensorflow.contrib.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_model(hidden_units, do_batch_norm=True, optimizer_cls=tf.train.AdamOptimizer, learning_rate=0.01):\n",
    "    model_info = {}\n",
    "\n",
    "    # Placeholders for data\n",
    "    #model_info['input'] = tf.placeholder(shape=(None, xTr.shape[1]), dtype=tf.float32)\n",
    "    model_info['input'] = tf.placeholder(shape=(None, hidden_units[0]), dtype=tf.float32)\n",
    "    #model_info['truth'] = tf.placeholder(shape=(None, yTr.shape[1]), dtype=tf.float32)\n",
    "    model_info['truth'] = tf.placeholder(shape=(None, hidden_units[-1]), dtype=tf.float32)\n",
    "\n",
    "    layers = []\n",
    "    #hidden_units = [xTr.shape[1], 200, 288]\n",
    "    for i, hidden_size in enumerate(hidden_units):\n",
    "        if i == 0:\n",
    "            layer = tf.contrib.layers.relu(model_info['input'], hidden_size)\n",
    "        else:\n",
    "            layer = tf.contrib.layers.relu(layer, hidden_size)\n",
    "        layers.append(layer)\n",
    "        if do_batch_norm:\n",
    "            layer = tf.contrib.layers.batch_norm(layer)\n",
    "            layers.append(layer)\n",
    "        if softmax:\n",
    "            layer = tf.contrib.layers.\n",
    "\n",
    "    model_info['layers'] = layers\n",
    "    model_info['prediction'] = layer\n",
    "\n",
    "    loss = tf.reduce_sum(tf.square(model_info['prediction'] - model_info['truth']))\n",
    "    #loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(model_info['prediction'], model_info['truth'], name=None))\n",
    "    model_info['loss'] = loss\n",
    "    \n",
    "    model_info['optimizer'] = optimizer_cls(learning_rate=learning_rate).minimize(loss)\n",
    "    \n",
    "    return model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_model(xTr, yTr, xTe, epochs=100, batch_size=128, learning_rate=0.01):\n",
    "    num_items = xTr.shape[0]\n",
    "    hidden_units = [xTr.shape[1], 1168, yTr.shape[1]]\n",
    "    losses=[]\n",
    "    \n",
    "    with tf.Graph().as_default():\n",
    "        model_info = create_model(hidden_units, learning_rate=learning_rate)\n",
    "        \n",
    "        init = tf.initialize_all_variables()\n",
    "        saver = tf.train.Saver()\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth=True\n",
    "        \n",
    "        with tf.Session(config=config) as sess:\n",
    "            sess.run(init)\n",
    "            print('Starting Training (%s images total)' % xTr.shape[0])\n",
    "            print(\"Architecture: %s\" % hidden_units)\n",
    "            \n",
    "            num_batches_per_epoch = int(num_items/batch_size)\n",
    "            for epoch in xrange(epochs):\n",
    "                for batch in xrange(num_batches_per_epoch):\n",
    "                    feed_dict = {}\n",
    "                    feed_dict[model_info['input']] = xTr[batch*batch_size:(batch+1)*batch_size]\n",
    "                    feed_dict[model_info['truth']] = yTr[batch*batch_size:(batch+1)*batch_size]\n",
    "\n",
    "                    # Add things you need\n",
    "                    sess.run(model_info['optimizer'], feed_dict=feed_dict)\n",
    "\n",
    "                    if batch%100 == 0:\n",
    "                        save_path = saver.save(sess, \"/tmp/model.ckpt\")\n",
    "                        # Print extra things\n",
    "                        loss = sess.run(model_info['loss'], feed_dict=feed_dict)\n",
    "                        print('(Epoch {}) Completed batch {} of {} (loss: {})'.format(epoch, batch, num_batches_per_epoch, loss))\n",
    "                        feed_dict = {}\n",
    "                        feed_dict[model_info['input']] = xTe\n",
    "                        yHat_output = sess.run(model_info['prediction'], feed_dict=feed_dict)\n",
    "                        losses.append(loss)\n",
    "                        \n",
    "            [precision, recall, f1score] = octave.evaluate(yTe.T, yHat_output.T, 5)\n",
    "            print(\"precision: \" + str(precision))\n",
    "            print(\"recall: \" + str(recall))\n",
    "            print(\"f1score: \" + str(f1score))\n",
    "    return [precision, recall, f1score, losses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p, r, f1, losses = train_model(xTr, yTr, xTe, epochs=10000, batch_size=1000, learning_rate=0.05)\n",
    "print p, r, f1\n",
    "print losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    epochs=100\n",
    "    batch_size=128\n",
    "    num_items = xTr.shape[0]\n",
    "    losses=[]\n",
    "    with tf.Graph().as_default():\n",
    "        model_info = create_model()\n",
    "        init = tf.initialize_all_variables()\n",
    "        saver = tf.train.Saver()\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth=True\n",
    "        with tf.Session(config=config) as sess:\n",
    "            sess.run(init)\n",
    "            print('Starting Training')\n",
    "            for epoch in range(epochs):\n",
    "                num_batches_per_epoch = int(num_items/batch_size)\n",
    "                for batch in range(num_batches_per_epoch):\n",
    "                    feed_dict = {}\n",
    "                    feed_dict[model_info['input']] = xTr[batch*batch_size:(batch+1)*batch_size]\n",
    "                    feed_dict[model_info['truth']] = yTr[batch*batch_size:(batch+1)*batch_size]\n",
    "\n",
    "                    # Add things you need\n",
    "                    sess.run(model_info['optimizer'], feed_dict=feed_dict)\n",
    "                    if batch%100 == 0:\n",
    "                        save_path = saver.save(sess, \"/tmp/model.ckpt\")\n",
    "                        # Print extra things\n",
    "\n",
    "                        loss = sess.run(model_info['loss'], feed_dict=feed_dict)\n",
    "\n",
    "                        print('Completed batch {} of {} with {} images (loss: {})'.format(batch, \n",
    "                                                                  (int(num_items/batch_size)),\n",
    "                                                                           xTr.shape[0],\n",
    "                                                                   loss))\n",
    "                        feed_dict = {}\n",
    "                        feed_dict[model_info['input']] = xTe\n",
    "                        yHat_output = sess.run(model_info['prediction'], feed_dict=feed_dict)\n",
    "            [precision, recall, f1score] = octave.evaluate(yTe.T, yHat_output.T, 5)\n",
    "            print(\"precision: \" + str(precision))\n",
    "            print(\"recall: \" + str(recall))\n",
    "            print(\"f1score: \" + str(f1score))\n",
    "    return [precision, recall, f1score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## tensorflow.contrib.learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build regressor\n",
    "regressor = tf.contrib.learn.TensorFlowDNNRegressor(hidden_units=[200, 150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regressor.fit(xTr, yTr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = regressor.predict(xTe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[precision, recall, f1score] = octave.evaluate(yTe.T, result.T, 5)\n",
    "print(\"precision: \" + str(precision))\n",
    "print(\"recall: \" + str(recall))\n",
    "print(\"f1score: \" + str(f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.contrib.learn.python.learn as learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg = learn.LinearRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reg.fit(xTr, yTr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = reg.predict(yTe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tflearn\n",
    "import tensorflow as tf\n",
    "# Linear Regression graph\n",
    "input_ = tflearn.input_data(shape=[None, xTr.shape[0], xTr.shape[1]])\n",
    "linear = tflearn.single_unit(input_)\n",
    "y_placeholder = tf.placeholder(tf.float64, shape=yTr.shape)\n",
    "regression = tflearn.regression(input_, placeholder=y_placeholder, optimizer='sgd', loss='mean_square', metric='R2', learning_rate=0.01)\n",
    "m = tflearn.DNN(regression)\n",
    "m.fit(xTr, yTr, n_epoch=1000, batch_size=500, show_metric=True, snapshot_epoch=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression via NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tflearn\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def w2vloss( feat, truths ):\n",
    "    return -tf.reduce_mean( truths * tf.log( feat ) + (1-truths)*tf.log(1-feat) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def imagemodel(input_size=2048, output_size=281, hidden_units=[]):\n",
    "    '''\n",
    "    imagemodel( input_size, output_size, hidden_units )\n",
    "    '''\n",
    "\n",
    "    labels = tf.placeholder(tf.float32, [None, output_size], name='labels')    \n",
    "    inputs = tf.placeholder(tf.float32, [None, input_size], name='inputs')\n",
    "    \n",
    "    # Iterate through the hidden units list and connect the graph\n",
    "    layer_i = inputs\n",
    "    for i, hidden in enumerate(hidden_units):\n",
    "        layer_i = tflearn.fully_connected(layer_i, hidden, activation='relu', name='fc'+str(i))\n",
    "    prediction = tflearn.fully_connected(layer_i, output_size, activation='sigmoid', name='output')\n",
    "\n",
    "    # Loss function and optimizer to be used\n",
    "    loss = w2vloss(prediction,labels)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "            \n",
    "    # Return actual variables\n",
    "    return inputs, prediction, labels, loss, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden_units = [200, ]\n",
    "inputs,preds,labels,loss,opt = imagemodel(output_size=yTe.shape[1], hidden_units = hidden_units)\n",
    "init_op = tf.initialize_all_variables()\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "bsize = 10000\n",
    "\n",
    "epoch_list = []\n",
    "loss_list = []\n",
    "\n",
    "print('Starting Training')\n",
    "for epoch in range(epochs):\n",
    "    for b in range(0,len(yTr),bsize):            \n",
    "        _, lossval = sess.run([opt,loss], feed_dict={inputs:xTr[b:b+bsize], labels:yTr[b:b+bsize]})\n",
    "    epoch_list.append(epoch+1)\n",
    "    loss_list.append(lossval)\n",
    "    sys.stdout.write(\"\\rEpoch {}/{}: loss={}\".format(epoch, epochs, lossval))\n",
    "plt.plot(epoch_list, loss_list)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yHat_output = sess.run(preds, feed_dict={inputs:xTe})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[precision, recall, f1score] = octave.evaluate(yTe.T, yHat_output.T, 5)\n",
    "print \"precision: \" + str(precision)\n",
    "print \"recall: \" + str(recall)\n",
    "print \"f1score: \" + str(f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "net = tflearn.input_data(shape=[None, 2048])\n",
    "net = tflearn.fully_connected(net, 200)\n",
    "#net = tflearn.fully_connected(net, 200)\n",
    "net = tflearn.fully_connected(net, 288, activation='softmax')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Define model\n",
    "model = tflearn.DNN(net)\n",
    "# Start training (apply gradient descent algorithm)\n",
    "model.fit(xTr, yTr, n_epoch=10, batch_size=500, show_metric=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#yHat = model.predict(xTe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Original Multihot scores\n",
    "# Precision: 0.396726436529\n",
    "# Recall: 0.211489266372\n",
    "# F1 Score: 0.275900088116\n",
    "\n",
    "# second step, filtered to original\n",
    "[precision, recall, f1score] = octave.evaluate(yTe.T, yHat.T, 10)\n",
    "print precision\n",
    "print recall\n",
    "print f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Original Multihot scores\n",
    "# Precision: 0.396726436529\n",
    "# Recall: 0.211489266372\n",
    "# F1 Score: 0.275900088116\n",
    "\n",
    "# nonlinear suppression, full chain\n",
    "[precision, recall, f1score] = octave.evaluate(yTe.T, yHat.T, 10)\n",
    "print precision\n",
    "print recall\n",
    "print f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Original Multihot scores\n",
    "# Precision: 0.396726436529\n",
    "# Recall: 0.211489266372\n",
    "# F1 Score: 0.275900088116\n",
    "\n",
    "# scaled multihot\n",
    "[precision, recall, f1score] = octave.evaluate(yTe.T, yHat.T, 10)\n",
    "print precision\n",
    "print recall\n",
    "print f1score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Predict labels for an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Randomly select an image\n",
    "i=671 #np.random.randint(0, yTe.shape[1])\n",
    "\n",
    "# Run example\n",
    "imname = os.path.join(img_dir, \"images\", \"images\", \"%s.jpg\" % test_ims[i])\n",
    "#imname = os.path.join(img_dir, \"ESP-ImageSet\", \"images\", \"%s\" % test_ims[i])\n",
    "print \"Looking at the \"+str(i)+\"th image: \"+imname\n",
    "im=plt.imread(imname)\n",
    "\n",
    "# Prediction\n",
    "#threshold = np.percentile(yHat[i], 99)\n",
    "#top_idxs = [idx for idx in yHat[i].argsort()[::-1] if yHat[i][idx] > threshold]\n",
    "#ypwords = [d_words[idx] for idx in top_idxs]\n",
    "ypwords = [\"%s (%s)\" % (D[idx], yHat[i][idx]) for idx in yHat[i].argsort()[::-1][:10]]\n",
    "#ypwords = [D[j] for j in yHat[i].argsort()[::-1] [ 0:(yHat[i]>0.2).sum() ] ]\n",
    "# Truth\n",
    "ytwords = [D[idx] for idx in np.nonzero(yTe[i])[0]]\n",
    "#ytwords = [D[j] for j in np.where(yTe[i] > 0.5)[0] ]\n",
    "plt.imshow(im)\n",
    "\n",
    "print 'Predicted: '\n",
    "for ypword in ypwords:\n",
    "    print \"\\t%s\" % ypword\n",
    "print 'Truth:     '+ ', '.join(ytwords)\n",
    "\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Randomly select an image\n",
    "i=671 #np.random.randint(0, yTe.shape[1])\n",
    "\n",
    "# Run example\n",
    "imname = os.path.join(img_dir, \"images\", \"images\", \"%s.jpg\" % test_ims[i])\n",
    "#imname = os.path.join(img_dir, \"ESP-ImageSet\", \"images\", \"%s\" % test_ims[i])\n",
    "print \"Looking at the \"+str(i)+\"th image: \"+imname\n",
    "im=plt.imread(imname)\n",
    "\n",
    "# Prediction\n",
    "#threshold = np.percentile(yHat[i], 99)\n",
    "#top_idxs = [idx for idx in yHat[i].argsort()[::-1] if yHat[i][idx] > threshold]\n",
    "#ypwords = [d_words[idx] for idx in top_idxs]\n",
    "ypwords = [\"%s (%s)\" % (D[idx], yHat[i][idx]) for idx in yHat[i].argsort()[::-1][:10]]\n",
    "#ypwords = [D[j] for j in yHat[i].argsort()[::-1] [ 0:(yHat[i]>0.2).sum() ] ]\n",
    "# Truth\n",
    "ytwords = [D[idx] for idx in np.nonzero(yTe[i])[0]]\n",
    "#ytwords = [D[j] for j in np.where(yTe[i] > 0.5)[0] ]\n",
    "plt.imshow(im)\n",
    "\n",
    "print 'Predicted: '\n",
    "for ypword in ypwords:\n",
    "    print \"\\t%s\" % ypword\n",
    "print 'Truth:     '+ ', '.join(ytwords)\n",
    "\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Randomly select an image\n",
    "i=671 #np.random.randint(0, yTe.shape[1])\n",
    "\n",
    "# Run example\n",
    "imname = os.path.join(img_dir, \"images\", \"images\", \"%s.jpg\" % test_ims[i])\n",
    "#imname = os.path.join(img_dir, \"ESP-ImageSet\", \"images\", \"%s\" % test_ims[i])\n",
    "print \"Looking at the \"+str(i)+\"th image: \"+imname\n",
    "im=plt.imread(imname)\n",
    "\n",
    "# Prediction\n",
    "#threshold = np.percentile(yHat[i], 99)\n",
    "#top_idxs = [idx for idx in yHat[i].argsort()[::-1] if yHat[i][idx] > threshold]\n",
    "#ypwords = [d_words[idx] for idx in top_idxs]\n",
    "ypwords = [\"%s (%s)\" % (D[idx], yHat[i][idx]) for idx in yHat[i].argsort()[::-1][:10]]\n",
    "#ypwords = [D[j] for j in yHat[i].argsort()[::-1] [ 0:(yHat[i]>0.2).sum() ] ]\n",
    "# Truth\n",
    "ytwords = [D[idx] for idx in np.nonzero(yTe[i])[0]]\n",
    "#ytwords = [D[j] for j in np.where(yTe[i] > 0.5)[0] ]\n",
    "plt.imshow(im)\n",
    "\n",
    "print 'Predicted: '\n",
    "for ypword in ypwords:\n",
    "    print \"\\t%s\" % ypword\n",
    "print 'Truth:     '+ ', '.join(ytwords)\n",
    "\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.stem(yHat[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expand vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nonlinearity2(arr, percentile=99, power=1):\n",
    "    for row in arr:\n",
    "        cutoff = np.percentile(row, percentile)\n",
    "        for idx, val in enumerate(row):\n",
    "            if val < cutoff:\n",
    "                row[idx] = 0\n",
    "    for idx, row in enumerate(arr):\n",
    "        arr[idx] = np.power(row, power)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_V = np.load(os.path.join(wordvecs_dir, \"w_v_16.npy\")) #construct_W(w2v_model, w2v_model.vocab, dtype=np.float16)\n",
    "#np.save(os.path.join(wordvecs_dir, \"w_v_16_kylez.npy\"), W_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#expansion_arr = nonlinearity2(np.dot(W_V.T, W_L), power=2)\n",
    "#expansion_arr = np.load(os.path.join(wordvecs_dir, \"expansion_arr.npy\"))\n",
    "#np.save(os.path.join(wordvecs_dir, \"expansion_arr.npy\"), expansion_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expanded = np.dot(yTr, nonlinearity2(np.dot(W_V.T, W_L), power=2).T)\n",
    "#expanded = np.load(os.path.join(wordvecs_dir, \"expanded_yHat.npy\"))\n",
    "#np.save(os.path.join(wordvecs_dir, \"expanded_yHat.npy\"), expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expanded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Randomly select an image\n",
    "i = 102 #np.random.randint(0, yTe.shape[1])\n",
    "\n",
    "# Run example\n",
    "imname = os.path.join(dataset_dir, \"images\", \"images\", \"%s.jpg\" % test_ims_full[i])\n",
    "print \"Looking at the \"+str(i)+\"th image: \"+imname\n",
    "im=plt.imread(imname)\n",
    "\n",
    "# Prediction\n",
    "#threshold = np.percentile(yHat[i], 99)\n",
    "#top_idxs = [idx for idx in yHat[i].argsort()[::-1] if yHat[i][idx] > threshold]\n",
    "#ypwords = [d_words[idx] for idx in top_idxs]\n",
    "ypwords = [w2v_model.vocab[idx] for idx in expanded[i].argsort()[::-1][:50]]\n",
    "#ypwords = [D[j] for j in yHat[i].argsort()[::-1] [ 0:(yHat[i]>0.2).sum() ] ]\n",
    "# Truth\n",
    "ytwords = [D[idx] for idx in np.nonzero(yTe[i])[0]]\n",
    "#ytwords = [D[j] for j in np.where(yTe[i] > 0.5)[0] ]\n",
    "plt.imshow(im)\n",
    "\n",
    "print 'Predicted: '+ ', '.join(ypwords)\n",
    "print 'Truth:     '+ ', '.join(ytwords)\n",
    "\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
